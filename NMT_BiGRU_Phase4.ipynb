{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_BiGRU-Phase4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LRWIB3zp5Sp"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=\"1\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWjBEeSSqBcj"
      },
      "source": [
        "# !git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "# !git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96S1GMlEqME2",
        "outputId": "9fba9a72-6b33-4692-d4ee-d7accba3bfcf"
      },
      "source": [
        "!pip install indic-nlp-library"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/51/f4e4542a226055b73a621ad442c16ae2c913d6b497283c99cae7a9661e6c/indic_nlp_library-0.71-py3-none-any.whl\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n",
            "Installing collected packages: morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.71 morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqqn-eLY2fCN",
        "outputId": "24f74cd5-72e7-4bb0-9330-a1f0f1fb2d15"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xVC_GyGqN4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133e6646-04c7-4b5a-f411-0615f09d3bfc"
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "from numpy import array"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHvKvzzLmPQZ",
        "outputId": "03524818-b1c1-4317-c056-989fe4fd39e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "import os\n",
        "import sys\n",
        "path = '/content/gdrive/My Drive/Dataset'\n",
        "os.chdir(path)\n",
        "print(os.getcwd())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EowiASGtRUQ"
      },
      "source": [
        "#Tokenizer for hindi language and then using the Devnagri normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_6KGlMtTW1"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize \n",
        "# from indicnlp.normalize.indic_normalize import DevanagariNormalizer\n",
        "# factory = DevanagariNormalizer()\n",
        "\n",
        "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
        "factory=IndicNormalizerFactory()\n",
        "normalizer=factory.get_normalizer(\"hi\",remove_nuktas=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lt_FNA7wBZ5"
      },
      "source": [
        "#Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(w):\n",
        "  w = re.sub(r\"([?.!,¿-])\", r\" \\1 \", w)\n",
        "  w = re.sub(r\"[().!,?-]+\", r\" \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w) #multiple spaces\n",
        "  w = w.lower().strip() #lowercase #Remove white spaces\n",
        "  return w"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShBUwHAZtyp5"
      },
      "source": [
        "#Using the contractions that can help in decoding\n",
        "\n",
        "abbr = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"here's\": \"here is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\",\n",
        "\"y'all\":\"you all\",\n",
        "\"let's\": \"let us\"\n",
        "}\n",
        "#Dictionary mapping contractions to word"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i5bxGeiwQmb"
      },
      "source": [
        "import csv\n",
        "data = []\n",
        "with open('train1.csv') as csvfile:\n",
        "    csvReader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in csvReader:\n",
        "        data.append([normalizeString(row[0]), normalizeString(row[1])])\n",
        "\n",
        "for l in data:\n",
        "  for w in l[1].split():\n",
        "    if(w in abbr.keys()):\n",
        "      #print(w)\n",
        "      l[1] = l[1].replace(w, abbr[w])\n",
        "      break\n",
        "\n",
        "data = data[1:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt4QUpM4wWbL"
      },
      "source": [
        "test = []\n",
        "with open('hindistatements-2.csv') as csvfile:\n",
        "    csvReader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in csvReader:\n",
        "        test.append([row[2]])\n",
        "\n",
        "test = test[1:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojc7h7GMtszj"
      },
      "source": [
        "#Word2index and Index2word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVY3K3p9tX6y"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "#helper class\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    #word -> index\n",
        "    self.word2index = {\"SOS\" : 0, \"EOS\" : 1, \"UNK\" : 2}\n",
        "    #count of each word to later replace rare words\n",
        "    self.word2count = {}\n",
        "    #index -> word\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\", 2 : \"UNK\"}\n",
        "    self.n_words = 3  # Count SOS, EOS, UNK tokens\n",
        "    self.max_seq_length=-1\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    #Find max seq length\n",
        "    sentence_split = sentence.split(' ')\n",
        "    if len(sentence_split) > self.max_seq_length:\n",
        "      self.max_seq_length = len(sentence_split)\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      # word to index mapping\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD0LrVNZuoIL"
      },
      "source": [
        "def clean_pairs(lines):\n",
        "  cleaned = list()\n",
        "  regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "  for pair in lines:\n",
        "    clean_pair = list()\n",
        "    \n",
        "    #cleaning hindi phrases\n",
        "    line = pair[0]\n",
        "    line = normalizer.normalize(line)\n",
        "    tokens = list()\n",
        "    for t in indic_tokenize.trivial_tokenize(line): \n",
        "        tokens.append(t)\n",
        "    line = tokens\n",
        "    line = [word for word in line if not re.search(r'\\d', word)]\n",
        "    line = [regex.sub('', w) for w in line]\n",
        "    #Replace the english characters\n",
        "    line = [re.sub(r\"[a-zA-Z]+?\\s\", \"\", w) for w in line]\n",
        "    line = [re.sub(r'[-.।|,?;:<>&$₹]+','',w) for w in line]\n",
        "    clean_pair.append(' '.join(line))\n",
        "\n",
        "    #cleaning english phrases\n",
        "    line = pair[1]\n",
        "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "    line = line.decode('UTF-8')\n",
        "    line = word_tokenize(line) \n",
        "    line = [regex.sub('', w) for w in line]\n",
        "    line = [re.sub(r\"[^a-zA-Z0-9?.!,¿\\-\\/]+\", \" \", w) for w in line]\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    clean_pair.append(' '.join(line))\n",
        "    \n",
        "    cleaned.append(clean_pair)\n",
        "  return array(cleaned)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94QK0fjvN7G"
      },
      "source": [
        "#Creating the languages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REjmiwBgvQvn"
      },
      "source": [
        "def readLangs(lang1, lang2):\n",
        "  #make Lang instances\n",
        "  input_lang = Lang(lang1)\n",
        "  output_lang = Lang(lang2)\n",
        "\n",
        "  return input_lang, output_lang"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll0p69RfxIzq"
      },
      "source": [
        "##Since there are a lot of example sentences and we want to train something quickly, we’ll trim the data set to only relatively short and simple sentences. Here the maximum length is 30 words (that includes ending punctuation) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDabbPznvzxL"
      },
      "source": [
        "MAX_LENGTH = 60\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) <= MAX_LENGTH and len(p[1].split(' ')) <= MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ccTCZ91cgk"
      },
      "source": [
        "#Finally preparing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYfMATs7xxs0"
      },
      "source": [
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(data))\n",
        "    pairs = clean_pairs(data)\n",
        "    pairs = filterPairs(pairs)\n",
        "\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    #Make word lists from sentences in pairs\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    # for pair in pairst:\n",
        "    #     input_lang.addSentence(pair[0])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs, test"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzpS1O8g15Ps"
      },
      "source": [
        "#Loading the data and creating language objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQFONBW-m1Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e2e842-02fa-495d-d7d0-a8cfd4a4c131"
      },
      "source": [
        "!head train1.csv"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hindi,english\n",
            "\"एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध से वापसी ली, उन्होंने वही काम किये जो कैदियों की कश्मकश के निदान हैं।\",\"In El Salvador, both sides that withdrew from their civil war took moves that had been proven to mirror a prisoner's dilemma strategy.\"\n",
            "मैं उनके साथ कोई लेना देना नहीं है.,I have nothing to do with them.\n",
            "-हटाओ रिक.,\"Fuck them, Rick.\"\n",
            "क्योंकि यह एक खुशियों भरी फ़िल्म है.,Because it's a happy film.\n",
            "The thought reaching the eyes...,The thought reaching the eyes...\n",
            "मैंने तुमे School से हटवा दिया .,I got you suspended.\n",
            "\"यह Vika, एक फूल है.\",\"It's a flower, Vika.\"\n",
            "पर मेरे लिए उसका यहुदी विरोधी होना उसके कार्यों को और भी प्रशंसनीय बनाता है क्योंकि उसके पास भी पक्षपात करने के वही कारण थे जो बाकी फौजियों के पास थे पर उसकी सच जानने और उसे बनाए रखने की प्रेरणा सबसे ऊपर थी,\"But personally, for me, the fact that Picquart was anti-Semitic actually makes his actions more admirable, because he had the same prejudices, the same reasons to be biased as his fellow officers, but his motivation to find the truth and uphold it trumped all of that.\"\n",
            "\"नहीं, नहीं, नहीं... ठीक है, हम उह हूँ... हम कार्ड का उपयोग करेंगे.\",\"No, no, no... fine, we'll uh... we'll use the card.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VjaugyM18Q8",
        "outputId": "71c71aff-46a3-4623-e03b-550627e85dd9"
      },
      "source": [
        "input_lang, output_lang, tr_pairs, pairst = prepareData('hindi', 'english')\n",
        "print(random.choice(tr_pairs))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 101322 sentence pairs\n",
            "Trimmed to 100926 sentence pairs\n",
            "Counted words:\n",
            "hindi 43447\n",
            "english 29398\n",
            "['आइवी लीग हाँ ' 'ivy league yeah']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVPH-EL_PCre",
        "outputId": "8470f4d5-d8da-43e8-9838-687d56633871"
      },
      "source": [
        "print(random.choice(tr_pairs))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['बम फेंक कर तो आप मित्र नहीं बना सकते हैं न', 'you do not make friends by bombing them right']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5NoaV_X7bTX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into train and test set\n",
        "pairs, val_pairs = train_test_split(tr_pairs, test_size=0.1, random_state = 12)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MM7zTpq1iS1",
        "outputId": "8999f710-bc29-4a97-8082-1f16279188b4"
      },
      "source": [
        "#Checking the pair of the test set size\n",
        "print('Test set size is : ', len(test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size is :  5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBpZTVNe7nIz"
      },
      "source": [
        "#Preparing the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTGrTTRAAuO8"
      },
      "source": [
        "###To train, for each pair we will need an input tensor (indexes of the words in the input sentence) and target tensor (indexes of the words in the target sentence). While creating these vectors we will append the EOS token to both sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSoNmn1W6Wfj"
      },
      "source": [
        "#Return a list of indexes, one for each word in the sentence\n",
        "def indexesFromSentence(lang, sentence):\n",
        "  indexes = []\n",
        "  for word in sentence.split(' '):\n",
        "    try:\n",
        "      indexes.append(lang.word2index[word])\n",
        "    except:\n",
        "      indexes.append(lang.word2index[\"UNK\"])\n",
        "  return indexes\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    #append the EOS token to both sequences - enables the model to define a distribution over sequences of all possible lengths.\n",
        "    indexes.append(EOS_token)\n",
        "    result = torch.LongTensor(indexes).view(-1,1).cuda()\n",
        "    return result\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    #input tensor - indexes of the words in the input sentence\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    #target tensor - indexes of the words in the target sentence\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpqMzFdJBve-",
        "outputId": "29f615dc-0cf5-48e5-8ae7-fb2606b775a5"
      },
      "source": [
        "TARGET_MAX_LENGTH = output_lang.max_seq_length + 1 #since the we are adding the EOS and SOS tokens\n",
        "print(TARGET_MAX_LENGTH)\n",
        "\n",
        "INPUT_MAX_LENGTH = input_lang.max_seq_length + 1 #since the we are adding the EOS and SOS tokens\n",
        "print(INPUT_MAX_LENGTH)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLKU4AmDKdr"
      },
      "source": [
        "#Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEnlpC7hDL1I"
      },
      "source": [
        "#Using teacher forcing causes it to converge faster\n",
        "teacher_forcing_ratio = 0.55\n",
        "#0.7\n",
        "clip = 5.0\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, input_max_length=INPUT_MAX_LENGTH, target_max_length = TARGET_MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    # Zero gradients of both optimizers - Resetting\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    target_tensor = target_tensor.to(device)\n",
        "\n",
        "    encoder_outputs = torch.zeros(input_max_length, encoder.hidden_size*2 , device=device) #Changed\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    #Pass the input sentence through the ENCODER and keep track of every output and the latest hidden state\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    #decoder is given the <SOS> token as its first input\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    #decoder hidden state (\"context vector\" of input sentence ) ; last hidden state of the encoder as its first hidden state\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    #since using the \"Teacher forcing\" - passing the actual output sentence as the input to decoder\n",
        "    if(random.random() < teacher_forcing_ratio):\n",
        "      use_teacher_forcing = True\n",
        "    else:\n",
        "      use_teacher_forcing = False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        #Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])     #For each decoder step (not the RNN step), compute the loss\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing via decoder_input\n",
        "\n",
        "    else:\n",
        "        #Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            #Get the index of best output word, and pass it as input to next decoder step\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    #backpropagate the loss \n",
        "    loss.backward()\n",
        "    #Clipping the gradient after the backward pass\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "    #apply the gradients using the optimization update step - update parameters\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()/target_length"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2RV_qJprXBS"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf48raqJDF3X"
      },
      "source": [
        "def trainIters(encoder, decoder, encoder_optimizer , decoder_optimizer, n_iters, print_every=10000, plot_every=10000):\n",
        "    # Keep track of time elapsed and running averages\n",
        "    start = time.time()\n",
        "    train_plot_losses = []\n",
        "    val_plot_losses = []\n",
        "    train_plot_bleu_scores = []\n",
        "    val_plot_bleu_scores = []\n",
        "    max_val_bleu_score = -1.0\n",
        "\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "    print_bleu_total = 0\n",
        "\n",
        "    training_pairs = []\n",
        "    training_sentence_pairs = []\n",
        "\n",
        "    # Get training data for this cycle\n",
        "    for i in range(n_iters):\n",
        "        sent_pair = random.choice(pairs)\n",
        "        training_pairs.append(tensorsFromPair(sent_pair))\n",
        "        #print(training_pairs)\n",
        "        training_sentence_pairs.append(sent_pair)\n",
        "    \n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        \n",
        "        # Run the train function\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, input_max_length = INPUT_MAX_LENGTH, target_max_length = TARGET_MAX_LENGTH )\n",
        "\n",
        "        _, sent_bleu_score = evaluate_pairs(encoder , decoder, pairs = [training_sentence_pairs[iter-1]] )\n",
        "        # Keep track of loss\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "        print_bleu_total += sent_bleu_score\n",
        "\n",
        "        #Compute the loss and Bleu score on both the training and development set\n",
        "        if iter % print_every == 0:\n",
        "            #print loss \n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) Training-set loss: %.4f' % (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "            #print BLEU score \n",
        "            train_bleu_score = print_bleu_total/print_every\n",
        "            print_bleu_total = 0\n",
        "            print('Training set Bleu score : %.4f ' %(train_bleu_score) )\n",
        "\n",
        "            ### --------------  Development set -------------###\n",
        "            val_loss , val_bleu_score = evaluate_pairs(encoder, decoder, val_pairs, show_translation = False)\n",
        "\n",
        "            #print loss \n",
        "            print('%s (%d %d%%) validation loss : %.4f' % (timeSince(start, iter / n_iters),iter, iter / n_iters * 100, val_loss))\n",
        "            \n",
        "            #print BLEU score\n",
        "            print('Validation set Bleu score : %.4f ' %(val_bleu_score) )\n",
        "\n",
        "            train_plot_losses.append(print_loss_avg)\n",
        "            val_plot_losses.append(val_loss)\n",
        "            train_plot_bleu_scores.append(train_bleu_score)\n",
        "            val_plot_bleu_scores.append(val_bleu_score)\n",
        "            print('')\n",
        "\n",
        "            if val_bleu_score >= max_val_bleu_score:\n",
        "                print('Saving the Best Model!!!')\n",
        "                max_val_bleu_score = val_bleu_score\n",
        "                BEST_SAVE_PATH = './encoder_decoder_model_best_50.tar'\n",
        "                torch.save({\n",
        "                            'encoder_state_dict': encoder.state_dict(),\n",
        "                            'attndecoder_state_dict': decoder.state_dict(),\n",
        "                            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),            \n",
        "                            'decoder_optimizer_state_dict' : decoder_optimizer.state_dict(),\n",
        "                            'criterion' : criterion.state_dict(),\n",
        "                            }, BEST_SAVE_PATH)\n",
        "    \n",
        "    history = {'train_loss' : train_plot_losses,\n",
        "               'val_loss': val_plot_losses,\n",
        "               'train_bleu_score' : train_plot_bleu_scores,\n",
        "               'val_bleu_score' : val_plot_bleu_scores\n",
        "               }\n",
        "\n",
        "    return history"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7g1Lo8aEFTf"
      },
      "source": [
        "#Building the Seq2Seq model\n",
        "\n",
        "\n",
        "\n",
        "*   Encoder\n",
        "*   Decoder\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_64u08VEJg1"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers=1, bidirectional=False, verbose = False, batch_size=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_var = hidden_size // 2 if bidirectional else hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.n_directions = 2 if bidirectional else 1\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        #An embedding to represent each word as a vector. Here our embedding length is 256\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "\n",
        "        # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        self.net = nn.GRU(hidden_size, hidden_size,num_layers=self.n_layers, bidirectional=self.bidirectional)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        #Forward propagate\n",
        "        output, hidden = self.net(output, hidden)\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Encoder input', input.shape)\n",
        "            print('Encoder output', output.shape)\n",
        "            print('Encoder hidden', hidden.shape)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.n_layers * self.n_directions, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-SbteKqGXwV"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDlV8pPrG1NX"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=INPUT_MAX_LENGTH, bidirectional=False):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        #Define parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.n_layers = n_layers\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        #Define layers\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        #decoder’s input and hidden state as ip to attn layer\n",
        "        self.attn = nn.Linear(self.hidden_size * 3, self.max_length)\n",
        "        # self.attn = GeneralAttn(hidden_size)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "\n",
        "        self.net = nn.GRU(self.hidden_size, self.hidden_size, num_layers=self.n_layers, bidirectional = self.bidirectional)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        #Embed - Get the embedding of the current input word (last output word)\n",
        "        embedded = self.embedding(input).view(1, 1, -1)   # S=1 x B x N\n",
        "        embedded = self.dropout(embedded)\n",
        "        self.hidden = hidden\n",
        "\n",
        "        # Calculate attention weights and apply to encoder outputs \n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0], hidden[1]), 1)), dim=1)\n",
        "\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
        "        # Combine embedded input word and attended context, run through RNN\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output, hidden = self.net(output, hidden)\n",
        "        # Final output layer\n",
        "        #Decode the hidden state of the last time step\n",
        "        # Normalize attention to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        #Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.n_layers * self.n_directions, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StTUBpmhG_MU"
      },
      "source": [
        "#Initializing the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDWfivF_G8K0"
      },
      "source": [
        "n_layers=2\n",
        "bidirectional = True\n",
        "hidden_size = 512\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, n_layers=n_layers, bidirectional=bidirectional).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, n_layers=n_layers, bidirectional=bidirectional).to(device)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JHRio8xHLaI"
      },
      "source": [
        "attn_decoder1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqC_5kpnHP-A"
      },
      "source": [
        "encoder1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1wXBXPIHSCD"
      },
      "source": [
        "#torch.no_grad removes the ability to perform backprop (that's fine as we're 'evaluating' our model)\n",
        "#but speeds up computation and reduces RAM consumption\n",
        "def evaluate(encoder, decoder, pair, input_max_length = INPUT_MAX_LENGTH, target_max_length = TARGET_MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #Prepare input and output tensors from the sentences\n",
        "        input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "        target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "        input_length = input_tensor.size()[0]\n",
        "        target_length = target_tensor.size()[0]\n",
        "\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "\n",
        "        #Run through encoder\n",
        "        #Generate the output sentence \n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(input_max_length, encoder.hidden_size*2, device=device) ## Changed\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0] #BUG '+'\n",
        "\n",
        "        #Create starting vectors for decoder\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  #SOS\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(input_max_length, input_max_length)\n",
        "\n",
        "        #Run through decoder\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            #store the decoder’s attention outputs\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            #Choose top word from output\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di]).item()\n",
        "\n",
        "            #if prediction is EOS, STOP!!!\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])   #else add to output string \n",
        "\n",
        "            #feed the decoder’s predictions back to itself  - # Next input is chosen word\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions, loss/target_length"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhT8JgI1Zdm2"
      },
      "source": [
        "learning_rate=0.001\n",
        "encoder_optimizer = optim.Adam(encoder1.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(attn_decoder1.parameters(), lr=learning_rate)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASUMB3fRZeem"
      },
      "source": [
        "encoder_optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2noNg788ZmmF"
      },
      "source": [
        "# criterion = nn.NLLLoss()\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkgiA_4XZojA"
      },
      "source": [
        "SAVE_PATH = './encoder_decoder_model_50.tar'\n",
        "\n",
        "# #### General checkpoint saving for inference or resuming training\n",
        "torch.save({\n",
        "            'encoder_state_dict': encoder1.state_dict(),\n",
        "            'attndecoder_state_dict': attn_decoder1.state_dict(),\n",
        "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),            \n",
        "            'decoder_optimizer_state_dict' : decoder_optimizer.state_dict(),\n",
        "            'criterion' : criterion.state_dict(),\n",
        "            }, SAVE_PATH)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Iy7v2zZwtx"
      },
      "source": [
        "#Importing for Bleu score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsvRUp51Zq0f"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIExtPZYZ-5M"
      },
      "source": [
        "def evaluate_pairs(encoder, decoder, pairs, show_translation = False):\n",
        "    n = len(pairs)\n",
        "    #Compute the average bleu score \n",
        "\n",
        "    bleu_scores_list = []\n",
        "    loss_list = []\n",
        "\n",
        "    for i in range(n):\n",
        "        # pair = random.choice(pairs)\n",
        "        pair = pairs[i]\n",
        "        \n",
        "        output_words, attentions, loss_value  = evaluate(encoder, decoder, pair, input_max_length = INPUT_MAX_LENGTH, target_max_length = TARGET_MAX_LENGTH )\n",
        "        output_sentence = ' '.join(output_words)\n",
        "\n",
        "        # f1.writelines(pair[0]+\",\"+output_sentence+\"\\n\")\n",
        "\n",
        "        if show_translation == True:\n",
        "            print('Hindi sentence : ', pair[0])\n",
        "            print('English sentence : ', pair[1])\n",
        "            print('Predicted sentence : ', output_sentence)\n",
        "\n",
        "        actual_sent_splitted = pair[1].split()\n",
        "        output_sent_splitted = output_sentence.split()\n",
        "        \n",
        "        smoothing_function = SmoothingFunction().method4\n",
        "        sent_bleu_score = 0.0\n",
        "\n",
        "        try:\n",
        "          sent_bleu_score = sentence_bleu(references = [actual_sent_splitted] , hypothesis = output_sent_splitted , weights = (0.25, 0.25, 0.25, 0.25), smoothing_function =  smoothing_function )\n",
        "        except:\n",
        "          pass\n",
        "\n",
        "        bleu_scores_list.append(sent_bleu_score)\n",
        "        loss_list.append(loss_value)\n",
        "\n",
        "    avg_bleu_score = np.nanmean(bleu_scores_list, )\n",
        "    avg_loss_value = np.nanmean(loss_list)\n",
        "    # print('Bleu score is : ', avg_bleu_score )\n",
        "    # print('')\n",
        "    return avg_loss_value, avg_bleu_score"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnNFkZ8LiFEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eefadc3-6936-4682-f176-ffc964c807cc"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "training_history = trainIters(encoder = encoder1, decoder = attn_decoder1, \n",
        "                              encoder_optimizer = encoder_optimizer, decoder_optimizer = decoder_optimizer,\n",
        "                              n_iters = 30000, print_every=1000)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1m 34s (- 45m 49s) (1000 3%) Training-set loss: 6.1500\n",
            "Training set Bleu score : 0.0614 \n",
            "4m 44s (- 137m 21s) (1000 3%) validation loss : 5.4866\n",
            "Validation set Bleu score : 0.0703 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "6m 35s (- 92m 17s) (2000 6%) Training-set loss: 5.9318\n",
            "Training set Bleu score : 0.0975 \n",
            "9m 34s (- 134m 3s) (2000 6%) validation loss : 4.8861\n",
            "Validation set Bleu score : 0.0755 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "11m 14s (- 101m 6s) (3000 10%) Training-set loss: 5.8477\n",
            "Training set Bleu score : 0.1136 \n",
            "13m 56s (- 125m 25s) (3000 10%) validation loss : 4.2079\n",
            "Validation set Bleu score : 0.0779 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "15m 38s (- 101m 42s) (4000 13%) Training-set loss: 5.6945\n",
            "Training set Bleu score : 0.1152 \n",
            "19m 0s (- 123m 36s) (4000 13%) validation loss : 5.8872\n",
            "Validation set Bleu score : 0.1170 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "20m 39s (- 103m 15s) (5000 16%) Training-set loss: 5.6021\n",
            "Training set Bleu score : 0.1188 \n",
            "24m 3s (- 120m 16s) (5000 16%) validation loss : 5.9291\n",
            "Validation set Bleu score : 0.1311 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "25m 45s (- 103m 0s) (6000 20%) Training-set loss: 5.7364\n",
            "Training set Bleu score : 0.1204 \n",
            "28m 43s (- 114m 52s) (6000 20%) validation loss : 5.0129\n",
            "Validation set Bleu score : 0.1072 \n",
            "\n",
            "30m 18s (- 99m 33s) (7000 23%) Training-set loss: 5.8813\n",
            "Training set Bleu score : 0.1244 \n",
            "33m 20s (- 109m 34s) (7000 23%) validation loss : 5.0391\n",
            "Validation set Bleu score : 0.1081 \n",
            "\n",
            "34m 56s (- 96m 5s) (8000 26%) Training-set loss: 5.7815\n",
            "Training set Bleu score : 0.1301 \n",
            "38m 1s (- 104m 33s) (8000 26%) validation loss : 5.1568\n",
            "Validation set Bleu score : 0.1181 \n",
            "\n",
            "39m 42s (- 92m 38s) (9000 30%) Training-set loss: 5.8484\n",
            "Training set Bleu score : 0.1399 \n",
            "42m 30s (- 99m 11s) (9000 30%) validation loss : 4.6057\n",
            "Validation set Bleu score : 0.0892 \n",
            "\n",
            "44m 4s (- 88m 8s) (10000 33%) Training-set loss: 5.7597\n",
            "Training set Bleu score : 0.1359 \n",
            "47m 4s (- 94m 8s) (10000 33%) validation loss : 4.9712\n",
            "Validation set Bleu score : 0.1111 \n",
            "\n",
            "48m 38s (- 84m 0s) (11000 36%) Training-set loss: 5.6989\n",
            "Training set Bleu score : 0.1367 \n",
            "51m 18s (- 88m 37s) (11000 36%) validation loss : 4.3476\n",
            "Validation set Bleu score : 0.0876 \n",
            "\n",
            "52m 52s (- 79m 18s) (12000 40%) Training-set loss: 5.7327\n",
            "Training set Bleu score : 0.1379 \n",
            "56m 8s (- 84m 12s) (12000 40%) validation loss : 5.7428\n",
            "Validation set Bleu score : 0.1350 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "57m 45s (- 75m 31s) (13000 43%) Training-set loss: 5.7228\n",
            "Training set Bleu score : 0.1419 \n",
            "60m 51s (- 79m 34s) (13000 43%) validation loss : 5.4254\n",
            "Validation set Bleu score : 0.1193 \n",
            "\n",
            "62m 25s (- 71m 20s) (14000 46%) Training-set loss: 5.8132\n",
            "Training set Bleu score : 0.1483 \n",
            "65m 27s (- 74m 48s) (14000 46%) validation loss : 5.2927\n",
            "Validation set Bleu score : 0.1176 \n",
            "\n",
            "67m 2s (- 67m 2s) (15000 50%) Training-set loss: 5.8017\n",
            "Training set Bleu score : 0.1445 \n",
            "69m 56s (- 69m 56s) (15000 50%) validation loss : 4.8371\n",
            "Validation set Bleu score : 0.1058 \n",
            "\n",
            "71m 30s (- 62m 34s) (16000 53%) Training-set loss: 5.7625\n",
            "Training set Bleu score : 0.1413 \n",
            "74m 57s (- 65m 35s) (16000 53%) validation loss : 6.2682\n",
            "Validation set Bleu score : 0.1440 \n",
            "\n",
            "Saving the Best Model!!!\n",
            "76m 36s (- 58m 34s) (17000 56%) Training-set loss: 5.9300\n",
            "Training set Bleu score : 0.1417 \n",
            "79m 31s (- 60m 49s) (17000 56%) validation loss : 4.8456\n",
            "Validation set Bleu score : 0.1051 \n",
            "\n",
            "81m 7s (- 54m 5s) (18000 60%) Training-set loss: 5.8776\n",
            "Training set Bleu score : 0.1465 \n",
            "84m 13s (- 56m 8s) (18000 60%) validation loss : 5.2059\n",
            "Validation set Bleu score : 0.1206 \n",
            "\n",
            "85m 48s (- 49m 40s) (19000 63%) Training-set loss: 5.7582\n",
            "Training set Bleu score : 0.1496 \n",
            "88m 50s (- 51m 26s) (19000 63%) validation loss : 5.3408\n",
            "Validation set Bleu score : 0.1108 \n",
            "\n",
            "90m 25s (- 45m 12s) (20000 66%) Training-set loss: 5.9226\n",
            "Training set Bleu score : 0.1446 \n",
            "93m 34s (- 46m 47s) (20000 66%) validation loss : 5.5395\n",
            "Validation set Bleu score : 0.1168 \n",
            "\n",
            "95m 6s (- 40m 45s) (21000 70%) Training-set loss: 5.8358\n",
            "Training set Bleu score : 0.1471 \n",
            "97m 28s (- 41m 46s) (21000 70%) validation loss : 3.6410\n",
            "Validation set Bleu score : 0.0768 \n",
            "\n",
            "99m 3s (- 36m 1s) (22000 73%) Training-set loss: 6.1083\n",
            "Training set Bleu score : 0.1427 \n",
            "102m 17s (- 37m 11s) (22000 73%) validation loss : 5.5598\n",
            "Validation set Bleu score : 0.1285 \n",
            "\n",
            "103m 55s (- 31m 37s) (23000 76%) Training-set loss: 5.9910\n",
            "Training set Bleu score : 0.1461 \n",
            "107m 12s (- 32m 37s) (23000 76%) validation loss : 5.9965\n",
            "Validation set Bleu score : 0.1358 \n",
            "\n",
            "108m 51s (- 27m 12s) (24000 80%) Training-set loss: 5.9233\n",
            "Training set Bleu score : 0.1460 \n",
            "111m 53s (- 27m 58s) (24000 80%) validation loss : 5.4355\n",
            "Validation set Bleu score : 0.1040 \n",
            "\n",
            "113m 29s (- 22m 41s) (25000 83%) Training-set loss: 5.9084\n",
            "Training set Bleu score : 0.1432 \n",
            "116m 31s (- 23m 18s) (25000 83%) validation loss : 5.1761\n",
            "Validation set Bleu score : 0.1110 \n",
            "\n",
            "118m 5s (- 18m 10s) (26000 86%) Training-set loss: 6.0680\n",
            "Training set Bleu score : 0.1440 \n",
            "120m 47s (- 18m 35s) (26000 86%) validation loss : 4.4418\n",
            "Validation set Bleu score : 0.0991 \n",
            "\n",
            "122m 22s (- 13m 35s) (27000 90%) Training-set loss: 6.0005\n",
            "Training set Bleu score : 0.1459 \n",
            "125m 42s (- 13m 58s) (27000 90%) validation loss : 6.1662\n",
            "Validation set Bleu score : 0.1292 \n",
            "\n",
            "127m 17s (- 9m 5s) (28000 93%) Training-set loss: 5.9446\n",
            "Training set Bleu score : 0.1477 \n",
            "130m 32s (- 9m 19s) (28000 93%) validation loss : 5.7424\n",
            "Validation set Bleu score : 0.1293 \n",
            "\n",
            "132m 7s (- 4m 33s) (29000 96%) Training-set loss: 6.0437\n",
            "Training set Bleu score : 0.1449 \n",
            "135m 29s (- 4m 40s) (29000 96%) validation loss : 6.1360\n",
            "Validation set Bleu score : 0.1262 \n",
            "\n",
            "137m 1s (- 0m 0s) (30000 100%) Training-set loss: 5.9147\n",
            "Training set Bleu score : 0.1386 \n",
            "140m 14s (- 0m 0s) (30000 100%) validation loss : 5.8799\n",
            "Validation set Bleu score : 0.1214 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPbYWD64iH4r"
      },
      "source": [
        "# device = torch.device('cpu')\n",
        "encoder_loaded = EncoderRNN(input_lang.n_words, hidden_size, n_layers=n_layers,  method='GRU', bidirectional=bidirectional)\n",
        "attn_decoder_loaded= AttnDecoderRNN(hidden_size, output_lang.n_words, n_layers=n_layers,method='GRU', bidirectional=bidirectional)\n",
        "#Load the dictionary from saved file\n",
        "BEST_SAVE_PATH = './encoder_decoder_model_best_50.tar'\n",
        "checkpoint = torch.load(BEST_SAVE_PATH)\n",
        "\n",
        "# load the state from dictionary \n",
        "encoder_loaded.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "attn_decoder_loaded.load_state_dict(checkpoint['attndecoder_state_dict'])\n",
        "\n",
        "encoder_loaded.train()\n",
        "attn_decoder_loaded.train()\n",
        "\n",
        "encoder_loaded = encoder_loaded.to(device)\n",
        "attn_decoder_loaded = attn_decoder_loaded.to(device)\n",
        "\n",
        "learning_rate=0.001\n",
        "encoder_optimizer_loaded = optim.Adam(encoder_loaded.parameters(), lr=learning_rate)\n",
        "decoder_optimizer_loaded = optim.Adam(attn_decoder_loaded.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83izU2uIiRvi"
      },
      "source": [
        "def evaluate1(encoder, decoder, sentence, max_length=INPUT_MAX_LENGTH,  target_max_length = TARGET_MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "#         input_tensor = input_tensor.to(device)\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(target_max_length, target_max_length)\n",
        "\n",
        "        for di in range(target_max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "#             decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "        #print(decoded_words)\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfbU0g60iX4w"
      },
      "source": [
        "f1= open(\"answer.txt\",'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rngJhNiMiatN"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, pairs1):\n",
        "    for i in range(len(pairs1)):\n",
        "        pair = pairs1[i]\n",
        "        # pair = random.choice(pairs)\n",
        "        #print('>',pair[0])\n",
        "        output_words = evaluate1(encoder, decoder,pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        f1.writelines(output_sentence+\"\\n\")\n",
        "        #print('<', output_sentence)\n",
        "        #print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KIFIWqcdipvI"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1, pairst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN2i422JisRU"
      },
      "source": [
        "pairst[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKw8XXOSitxJ"
      },
      "source": [
        "test_loss , test_bleu_score = evaluate_pairs(encoder = encoder1, decoder = attn_decoder1, pairs = pairst[5:15], show_translation = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osi9jeXiixMP"
      },
      "source": [
        "f1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDIpDjrhiy5T"
      },
      "source": [
        "SAVE_PATH = './encoder_decoder_model_best_50.tar'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31sJufMji1Nq"
      },
      "source": [
        "# ##  Load the dictionary from saved file\n",
        "checkpoint = torch.load(SAVE_PATH)\n",
        "\n",
        "# load the state from dictionary \n",
        "encoder1.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "attn_decoder1.load_state_dict(checkpoint['attndecoder_state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7at3RSAVi4H9"
      },
      "source": [
        "encoder1.train().to(device)\n",
        "attn_decoder1.train().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F1iGa_ri5UP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}