{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimus_Prime-v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wce2JPOtzija"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=\"1\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1p3Ryrj2wyE"
      },
      "source": [
        "#!pip install torchtext==0.6.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRShdK1UzrMy",
        "outputId": "f2b9f43a-a010-422d-c805-0b6e3fbf43e9"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.data import Field,BucketIterator,TabularDataset\n",
        "print(torchtext.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "from unicodedata import normalize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from numpy import array\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n",
            "0.6.0\n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MddgEo85z9mA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abea8053-db2d-48a1-b2cb-acbe05ed32bb"
      },
      "source": [
        "!pip install indic-nlp-library"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.7/dist-packages (0.71)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (1.19.5)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.7/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->indic-nlp-library) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW2S3Z981aO7"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.hi import Hindi"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxbBIQt9ztNC"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYxPFGZ95z-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d65cae-afcc-4863-9a6d-aa646ff174b1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_GhDRXj1kWT"
      },
      "source": [
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxHzcEze0Vkm"
      },
      "source": [
        "# tokenizing hindi data \n",
        "def tokenize_hin(article):\n",
        "    nlp = Hindi()\n",
        "    doc = nlp(article)\n",
        "    tokens = [token.text for token in doc]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# loading the simple spacy tokenizer for english \n",
        "def tokenize_en(sentence):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(sentence)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5QYzRdKchW0"
      },
      "source": [
        "# Special Tokens\n",
        "BOS_WORD = '<sos?'\n",
        "EOS_WORD = '<eos>'\n",
        "BLANK_WORD = \"<blank>\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyxemkqwDueK"
      },
      "source": [
        "HI_TEXT = Field(tokenize=tokenize_hin, pad_token=BLANK_WORD)\n",
        "EN_TEXT = Field(tokenize=tokenize_en, init_token = \"<sos>\", eos_token = \"<eos>\", pad_token=BLANK_WORD)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML_wq3adD45J"
      },
      "source": [
        "# associate the text in the 'English' column with the EN_TEXT field\n",
        "data_fields = [('hindi', HI_TEXT), ('english', EN_TEXT)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBFx-dX7ExTc",
        "outputId": "9bf06d86-9bad-485a-88ed-802ea0624f6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsCuFXGYE9k8",
        "outputId": "ff5392ac-4bbc-48d2-faee-136fe2a4ec92"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "path = '/content/gdrive/My Drive/'\n",
        "os.chdir(path)\n",
        "print(os.getcwd())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hmZeli4r7FD"
      },
      "source": [
        "df = pd.read_csv(\"Dataset/train.csv\")\n",
        "#adding columns with the length of the sentences\n",
        "df['hin_len'] = df['hindi'].str.count(' ')\n",
        "df['eng_len'] = df['english'].str.count(' ')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMKLJKcsr5YA",
        "outputId": "d9972103-5f9e-4af1-b75b-e4d58d83cdce"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101322, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnMap3xnsP1Q"
      },
      "source": [
        "#remove long sentences and others with large difference in the translations \n",
        "df = df.query('hin_len < 80 & eng_len < 80')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kCs8nsgset7"
      },
      "source": [
        "df = df.drop([df.columns[0]], axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "cXzDlbiysf9T",
        "outputId": "cbad89de-405f-4d2f-f888-8c27190166b2"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "      <th>hin_len</th>\n",
              "      <th>eng_len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...</td>\n",
              "      <td>In El Salvador, both sides that withdrew from ...</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>मैं उनके साथ कोई लेना देना नहीं है.</td>\n",
              "      <td>I have nothing to do with them.</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-हटाओ रिक.</td>\n",
              "      <td>Fuck them, Rick.</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>क्योंकि यह एक खुशियों भरी फ़िल्म है.</td>\n",
              "      <td>Because it's a happy film.</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "      <td>The thought reaching the eyes...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               hindi  ... eng_len\n",
              "0  एल सालवाडोर मे, जिन दोनो पक्षों ने सिविल-युद्ध...  ...      22\n",
              "1                मैं उनके साथ कोई लेना देना नहीं है.  ...       6\n",
              "2                                         -हटाओ रिक.  ...       2\n",
              "3               क्योंकि यह एक खुशियों भरी फ़िल्म है.  ...       4\n",
              "4                   The thought reaching the eyes...  ...       4\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zr61-hUsXUr"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,val=train_test_split(df,test_size=0.1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4MynnZDsbPK",
        "outputId": "965716ac-03b0-4859-91e4-bac3e89c5be6"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(91112, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er5W1Yxfh1pZ"
      },
      "source": [
        "train.to_csv(\"Dataset/train1.csv\", index=False)\n",
        "val.to_csv(\"Dataset/val1.csv\", index=False)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-7pSP-R11O-"
      },
      "source": [
        "train,val = TabularDataset.splits(path='Dataset/', train='train1.csv', validation='val1.csv', format='csv', fields=data_fields, skip_header = True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khVMyPOFd_bL",
        "outputId": "0c40c9a8-1c95-4566-e531-d8819c31d4b8"
      },
      "source": [
        "for i, example in enumerate([(x.hindi,x.english) for x in train[0:5]]):\n",
        "  print(f\"Example_{i}:{example}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example_0:(['हम', 'भगवान', 'Tyrion', 'के', 'चैम्बर', 'के', 'लिए', 'मोमबत्ती', 'की', 'बहुत', 'आवश्यकता', 'होगी', '।'], ['We', \"'ll\", 'need', 'plenty', 'of', 'candles', 'for', 'Lord', 'Tyrion', \"'s\", 'chamber', '.'])\n",
            "Example_1:(['और', 'मुझे', 'लगता', 'है', ',', 'इन', 'दिनों', 'समाचार', 'में', 'यह', 'बहुत', 'कुछ', 'चलता', 'है', '।'], ['And', 'this', 'is', 'something', 'that', \"'s\", ',', 'I', 'think', ',', 'in', 'the', 'news', 'a', 'lot', 'these', 'days', '.'])\n",
            "Example_2:(['तकनीशियन', ',', 'यह', 'आपके', 'नियंत्रण', 'है'], ['Technician', ',', 'this', 'is', 'your', 'Control', '.'])\n",
            "Example_3:(['याद', 'रखें', ',', 'यही', 'हम', 'पूरा', 'करना', 'चाहते', 'हैं', '।'], ['Remember', ',', 'that', \"'s\", 'what', 'we', 'want', 'to', 'have', 'accomplished', '.'])\n",
            "Example_4:(['अब', 'समापन', 'में', ',', 'मुझे', 'लगता', 'है', 'कि', 'हम', 'सब', 'को', 'इस', 'बारे', 'में', 'सोचने', 'की', 'जरूरत', 'है', 'अगर', 'हम', 'चाहते', 'हैं', 'कि', 'यह', 'वास्तविकता', 'बने', '-', 'और', 'यदि', 'हां', ',', 'यह', 'जीवन', 'की', 'परिभाषा', 'और', 'उसके', 'बाद', 'के', 'लिए', 'क्या', 'मतलब', 'रखता', 'है', '.'], ['Now', 'in', 'closing', ',', 'I', 'think', 'what', 'we', 'all', 'need', 'to', 'be', 'thinking', 'about', 'is', 'if', 'we', 'want', 'that', 'to', 'become', 'our', 'reality', '--', 'and', 'if', 'so', ',', 'what', 'it', 'means', 'for', 'a', 'definition', 'of', 'life', 'and', 'everything', 'that', 'comes', 'after', 'it', '.'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4UCkB6iG2Va"
      },
      "source": [
        "HI_TEXT.build_vocab(train, val)\n",
        "EN_TEXT.build_vocab(train, val)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp4aZ1qTHM-z"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "# Create iterators to process text in batches of approx. the same length by sorting on sentence lengths\n",
        "train_iter = data.BucketIterator(train, batch_size=BATCH_SIZE, repeat=False, sort_key=lambda x: len(x.hindi), shuffle=True )\n",
        "val_iter = data.BucketIterator(val, batch_size=1, repeat=False, sort_key=lambda x: len(x.hindi))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r0Chs-pHTJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b4148b-8826-4837-c6eb-bd5a3a04f092"
      },
      "source": [
        "batch = next(iter(train_iter))\n",
        "src_matrix = batch.hindi.T\n",
        "print(src_matrix, src_matrix.size())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  18,   10,  746,  ...,    1,    1,    1],\n",
            "        [  13,  231,   51,  ...,    1,    1,    1],\n",
            "        [ 390, 1991,    8,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [  42,   85,  624,  ...,    1,    1,    1],\n",
            "        [4475,   73,    1,  ...,    1,    1,    1],\n",
            "        [  19,  179,   67,  ...,    1,    1,    1]]) torch.Size([64, 53])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0-6SIvUnh5q",
        "outputId": "2b7c4a1c-34bc-4c09-9192-e46201cefe74"
      },
      "source": [
        "trg_matrix = batch.english.T\n",
        "print(trg_matrix, trg_matrix.size())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    2,   101,    17,  ...,     1,     1,     1],\n",
            "        [    2,     8,   104,  ...,     1,     1,     1],\n",
            "        [    2,  2079,  7578,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    2,    78,    30,  ...,     1,     1,     1],\n",
            "        [    2, 18296,    75,  ...,     1,     1,     1],\n",
            "        [    2,    36,   218,  ...,     1,     1,     1]]) torch.Size([64, 51])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT3ZwDVa9vXl"
      },
      "source": [
        "import math"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geFDcoxT9Ebp"
      },
      "source": [
        "# class TransformerModel(nn.Module):\n",
        "#   def __init__(self, vocab_size_en,vocab_size_fr, dim_input, nos_head, fd_frwd, nlayers, dropout=0.5):\n",
        "#     super(TransformerModel, self).__init__()\n",
        "#     from torch.nn import TransformerEncoderLayer,TransformerEncoder,TransformerDecoder,TransformerDecoderLayer, Embedding\n",
        "#     self.src_mask=None\n",
        "#     self.embed_en =Embedding(vocab_size_en,dim_input)\n",
        "#     self.pos_encoder=PositionalEncoding(dim_input)\n",
        "#     encoder_layers=TransformerEncoderLayer(dim_input,nos_head,fd_frwd,dropout)\n",
        "#     self.encoder=TransformerEncoder(encoder_layers,nlayers)\n",
        "#     self.dim_input=dim_input\n",
        "#     self.nlayers=nlayers\n",
        "#     self.embed_fr=Embedding(vocab_size_fr,dim_input)\n",
        "#     self.pos_decoder=PositionalEncoding(dim_input)\n",
        "#     dec_layers=TransformerDecoderLayer(dim_input,nos_head,fd_frwd,dropout)\n",
        "#     self.decoder=TransformerDecoder(dec_layers,nlayers)\n",
        "#     self.decoder_out_layer=nn.Linear(dim_input,vocab_size_fr)\n",
        "#     self.output_final=nn.Softmax()\n",
        "#     self.init_weights()\n",
        "#   def _generate_square_subsequent_mask(self, sz):\n",
        "#     mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "#     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "#     return mask\n",
        "  \n",
        "#   def init_weights(self):\n",
        "#     initrange = 0.1\n",
        "#     self.embed_en.weight.data.uniform_(-initrange, initrange)\n",
        "#     self.embed_fr.weight.data.uniform_(-initrange, initrange)\n",
        "#     self.decoder_out_layer.bias.data.zero_()\n",
        "#     self.decoder_out_layer.weight.data.uniform_(-initrange, initrange)\n",
        "  \n",
        "#   def forward(self,inp,target):\n",
        "#     inp=self.embed_en(inp)\n",
        "#     inp=self.pos_encoder(inp)\n",
        "#     encoder_output = self.encoder(inp)\n",
        "#     #print(encoder_output.shape)\n",
        "#     #print(self.src_mask == None or self.src_mask.size(0) != len(target))\n",
        "#     if (self.src_mask == None or self.src_mask.size(0) != len(target)): \n",
        "#       device = target.device\n",
        "#       #print(len(target))\n",
        "#       mask = self._generate_square_subsequent_mask(len(target)).to(device)\n",
        "#       src_mask = mask\n",
        "#       #print(src_mask)\n",
        "#     target = self.embed_fr(target) \n",
        "#     target = self.pos_decoder(target)\n",
        "#     #print(target.shape)\n",
        "#     output = self.decoder(target,encoder_output,src_mask,memory_mask=None,tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
        "#     output = self.decoder_out_layer(output)\n",
        "#     out=self.output_final(output)\n",
        "#     return out"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxSPs-D99Qr3"
      },
      "source": [
        "# vocab_size_hi=len(HI_TEXT.vocab.stoi)\n",
        "# vocab_size_en=len(EN_TEXT.vocab.stoi)\n",
        "# dim_input=256\n",
        "# nos_head=4\n",
        "# nlayers=4\n",
        "# fd_frwd=512"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGb5ug3p9cmr"
      },
      "source": [
        "# class PositionalEncoding1(nn.Module):\n",
        "#     def __init__(self, d_model, max_seq_len=500):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         pe = torch.zeros((max_seq_len, d_model))\n",
        "#         position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#         self.register_buffer('pe', pe)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.pe[:x.size(0), :]\n",
        "#         return (x)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1mFMDVfomw-"
      },
      "source": [
        "from torch import Tensor\n",
        "import torchvision\n",
        "\n",
        "import copy\n",
        "from typing import Optional, Any\n",
        "from torch.nn.init import xavier_uniform_\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G53uUZqbobFb"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "    \n",
        "class MyTransformer(nn.Module):\n",
        "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
        "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
        "                 activation: str = \"relu\",source_vocab_length: int = 60000,target_vocab_length: int = 60000) -> None:\n",
        "        super(MyTransformer, self).__init__()\n",
        "        self.source_embedding = nn.Embedding(source_vocab_length, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "        encoder_norm = nn.LayerNorm(d_model)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
        "        self.target_embedding = nn.Embedding(target_vocab_length, d_model)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "        decoder_norm = nn.LayerNorm(d_model)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
        "        self.out = nn.Linear(512, target_vocab_length)\n",
        "        self._reset_parameters()\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, tgt_mask: Optional[Tensor] = None,\n",
        "                memory_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None,\n",
        "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
        "        if src.size(1) != tgt.size(1):\n",
        "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
        "        src = self.source_embedding(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
        "        tgt = self.target_embedding(tgt)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
        "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                              memory_key_padding_mask=memory_key_padding_mask)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                xavier_uniform_(p)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk-AtiB1qSrR"
      },
      "source": [
        "source_vocab_length = len(HI_TEXT.vocab)\n",
        "target_vocab_length = len(EN_TEXT.vocab)\n",
        "\n",
        "model = MyTransformer(source_vocab_length=source_vocab_length,target_vocab_length=target_vocab_length)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "model = model.cuda()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpkP0QYqq04_"
      },
      "source": [
        "use_gpu = True"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OptP4SLEqjPO"
      },
      "source": [
        "def train(train_iter, val_iter, model, optim, num_epochs,use_gpu=True): \n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        valid_loss = 0\n",
        "        # Train model\n",
        "        model.train()\n",
        "        for i, batch in enumerate(train_iter):\n",
        "            src = batch.hindi.cuda() if use_gpu else batch.hindi\n",
        "            trg = batch.english.cuda() if use_gpu else batch.english\n",
        "            #change to shape (bs , max_seq_len)\n",
        "            src = src.transpose(0,1)\n",
        "            #change to shape (bs , max_seq_len+1) , Since right shifted\n",
        "            trg = trg.transpose(0,1)\n",
        "            trg_input = trg[:, :-1]\n",
        "            targets = trg[:, 1:].contiguous().view(-1)\n",
        "            src_mask = (src != 0)\n",
        "            src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
        "            src_mask = src_mask.cuda() if use_gpu else src_mask\n",
        "            trg_mask = (trg_input != 0)\n",
        "            trg_mask = trg_mask.float().masked_fill(trg_mask == 0, float('-inf')).masked_fill(trg_mask == 1, float(0.0))\n",
        "            trg_mask = trg_mask.cuda() if use_gpu else trg_mask\n",
        "            size = trg_input.size(1)\n",
        "            #print(size)\n",
        "            np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
        "            np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
        "            np_mask = np_mask.cuda() if use_gpu else np_mask   \n",
        "            # Forward, backprop, optimizer\n",
        "            optim.zero_grad()\n",
        "            preds = model(src.transpose(0,1), trg_input.transpose(0,1), tgt_mask = np_mask)#, src_mask = src_mask)#, tgt_key_padding_mask=trg_mask)\n",
        "            preds = preds.transpose(0,1).contiguous().view(-1, preds.size(-1))\n",
        "            loss = F.cross_entropy(preds,targets, ignore_index=0,reduction='sum')\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            train_loss += loss.item()/BATCH_SIZE\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_iter):\n",
        "                src = batch.hindi.cuda() if use_gpu else batch.hindi\n",
        "                trg = batch.english.cuda() if use_gpu else batch.english\n",
        "                #change to shape (bs , max_seq_len)\n",
        "                src = src.transpose(0,1)\n",
        "                #change to shape (bs , max_seq_len+1) , Since right shifted\n",
        "                trg = trg.transpose(0,1)\n",
        "                trg_input = trg[:, :-1]\n",
        "                targets = trg[:, 1:].contiguous().view(-1)\n",
        "                src_mask = (src != 0)\n",
        "                src_mask = src_mask.float().masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0.0))\n",
        "                src_mask = src_mask.cuda() if use_gpu else src_mask\n",
        "                trg_mask = (trg_input != 0)\n",
        "                trg_mask = trg_mask.float().masked_fill(trg_mask == 0, float('-inf')).masked_fill(trg_mask == 1, float(0.0))\n",
        "                trg_mask = trg_mask.cuda() if use_gpu else trg_mask\n",
        "                size = trg_input.size(1)\n",
        "                #print(size)\n",
        "                np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
        "                np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
        "                np_mask = np_mask.cuda() if use_gpu else np_mask\n",
        "\n",
        "                preds = model(src.transpose(0,1), trg_input.transpose(0,1), tgt_mask = np_mask)#, src_mask = src_mask)#, tgt_key_padding_mask=trg_mask)\n",
        "                preds = preds.transpose(0,1).contiguous().view(-1, preds.size(-1))         \n",
        "                loss = F.cross_entropy(preds,targets, ignore_index=0,reduction='sum')\n",
        "                valid_loss += loss.item()/1\n",
        "            \n",
        "        # Log after each epoch\n",
        "        print(f'''Epoch [{epoch+1}/{num_epochs}] complete. Train Loss: {train_loss/len(train_iter):.3f}. Val Loss: {valid_loss/len(val_iter):.3f}''')\n",
        "        \n",
        "        #Save best model till now:\n",
        "        if valid_loss/len(val_iter)<min(valid_losses,default=1e9): \n",
        "            print(\"saving state dict\")\n",
        "            torch.save(model.state_dict(), f\"checkpoint_best_epoch.pt\")\n",
        "        \n",
        "        train_losses.append(train_loss/len(train_iter))\n",
        "        valid_losses.append(valid_loss/len(val_iter))\n",
        "        \n",
        "        # Check Example after each epoch:\n",
        "        sentences = [\"तुमने उनकी बात सुनी\"]\n",
        "        for sentence in sentences:\n",
        "            print(f\"Original Sentence: {sentence}\")\n",
        "            print(f\"Translated Sentence: {greeedy_decode_sentence(model,sentence)}\")\n",
        "    return train_losses,valid_losses"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-WE_I-dqs2f"
      },
      "source": [
        "def greeedy_decode_sentence(model,sentence):\n",
        "    model.eval()\n",
        "    sentence = HI_TEXT.preprocess(sentence)\n",
        "    indexed = []\n",
        "    for tok in sentence:\n",
        "        if HI_TEXT.vocab.stoi[tok] != 0 :\n",
        "            indexed.append(HI_TEXT.vocab.stoi[tok])\n",
        "        else:\n",
        "            indexed.append(0)\n",
        "    sentence = Variable(torch.LongTensor([indexed])).cuda()\n",
        "    trg_init_tok = EN_TEXT.vocab.stoi[BOS_WORD]\n",
        "    trg = torch.LongTensor([[trg_init_tok]]).cuda()\n",
        "    translated_sentence = \"\"\n",
        "    maxlen = 80\n",
        "    for i in range(maxlen):\n",
        "        size = trg.size(0)\n",
        "        np_mask = torch.triu(torch.ones(size, size)==1).transpose(0,1)\n",
        "        np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0.0))\n",
        "        np_mask = np_mask.cuda()\n",
        "        pred = model(sentence.transpose(0,1), trg, tgt_mask = np_mask)\n",
        "        add_word = EN_TEXT.vocab.itos[pred.argmax(dim=2)[-1]]\n",
        "        translated_sentence+=\" \"+add_word\n",
        "        if add_word==EOS_WORD:\n",
        "            break\n",
        "        trg = torch.cat((trg,torch.LongTensor([[pred.argmax(dim=2)[-1]]]).cuda()))\n",
        "        #print(trg)\n",
        "    return translated_sentence"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njk18XNpqnC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "12937471-131e-4408-f5ab-44ce3e73d23f"
      },
      "source": [
        "train_losses,valid_losses = train(train_iter, val_iter, model, optim, 20)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20] complete. Train Loss: 85.280. Val Loss: 68.915\n",
            "saving state dict\n",
            "Original Sentence: तुमने उनकी बात सुनी\n",
            "Translated Sentence:  I 'm a little time , I 'm a little time , I 'm a little way . <eos>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-62b4d6ae3265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-b9bff023ae4b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_iter, val_iter, model, optim, num_epochs, use_gpu)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 892.00 MiB (GPU 0; 11.17 GiB total capacity; 8.31 GiB already allocated; 862.81 MiB free; 9.90 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trn0QsJA9f5s"
      },
      "source": [
        "#model=TransformerModel(vocab_size_hi,vocab_size_en,dim_input,nos_head,fd_frwd,nlayers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3ASWUZA9zQK"
      },
      "source": [
        "# import time\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# lr = 5.0 # learning rate\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) \n",
        "# start_time=time.time()\n",
        "# tot_loss=0\n",
        "# for i,batch in enumerate(train_iter):\n",
        "#   #print(\"batch\",i)\n",
        "#   tr_hi=batch.hindi\n",
        "#   tr_target=batch.english\n",
        "#   optimizer.zero_grad()                     \n",
        "#   model.train() \n",
        "#   output = model(tr_hi,tr_target)\n",
        "#   loss = criterion(output.view(-1, vocab_size_en), tr_target.view(-1))\n",
        "#   loss.backward()\n",
        "#   optimizer.step()\n",
        "#   log_interval = 200\n",
        "#   tot_loss+=loss\n",
        "#   #print the loss and time per 200 batches\n",
        "#   if i%2 ==0 :\n",
        "#     elapsed = time.time() - start_time\n",
        "#     print(\"loss:\",tot_loss,\"\\t\",\"time:\",elapsed)\n",
        "#     tot_loss=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRRFj9-YpTy0",
        "outputId": "ddf3d2ab-089d-4d4f-9752-08d8bd30cf59"
      },
      "source": [
        "print(greeedy_decode_sentence(model,\"तुमने उनकी बात सुनी\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " <unk> परेशान l'मेरे <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}